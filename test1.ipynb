{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find data for the following NCT IDs:\n",
      "['NCT0580550']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0e15e0863e9464686e55909aa18e2d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(FileUpload(value={}, accept='.csv', description='Upload CSV'), Textarea(value='', description='â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import openai\n",
    "import asyncio\n",
    "import nltk\n",
    "from IPython.display import display, clear_output, FileLink\n",
    "import ipywidgets as widgets\n",
    "from tqdm.notebook import tqdm\n",
    "import logging\n",
    "import pandas as pd\n",
    "import time\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "import io\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "client=AsyncOpenAI()\n",
    "\n",
    "# Securely prompt the user for the OpenAI API Key\n",
    "openai_api_key = getpass(\"Enter your OpenAI API Key: \")\n",
    "\n",
    "# Set the OpenAI API key\n",
    "openai.api_key = openai_api_key\n",
    "\n",
    "# Initialize token usage tracking\n",
    "token_usage = {\n",
    "    'gpt-4': {\n",
    "        'prompt_tokens': 0,\n",
    "        'completion_tokens': 0,\n",
    "        'total_tokens': 0\n",
    "    },\n",
    "    'gpt-3.5-turbo': {\n",
    "        'prompt_tokens': 0,\n",
    "        'completion_tokens': 0,\n",
    "        'total_tokens': 0\n",
    "    },\n",
    "    'text-embedding-ada-002': {\n",
    "        'total_tokens': 0\n",
    "    }\n",
    "}\n",
    "\n",
    "# Apply nest_asyncio to allow nested event loops in Jupyter Notebook\n",
    "try:\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "except ImportError:\n",
    "    !pip install nest_asyncio\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "\n",
    "# Download necessary NLTK data quietly\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "# Configure logging for debugging and monitoring\n",
    "logging.basicConfig(level=logging.INFO, filename='clinical_trials.log', filemode='a',\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Function to clean extracted text by removing escape characters and unnecessary whitespace\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans and formats text by removing escape characters and unnecessary whitespace.\n",
    "    \"\"\"\n",
    "    if isinstance(text, str):\n",
    "        return text.replace('\\\\>', '>').replace('\\\\<', '<').replace('\\\\', '').strip()\n",
    "    return text\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def get_broader_terms(condition):\n",
    "    synsets = wn.synsets(condition, pos=wn.NOUN)\n",
    "    broader_terms = set()\n",
    "    for syn in synsets:\n",
    "        for hypernym in syn.hypernyms():\n",
    "            broader_terms.update(lemma.name().replace('_', ' ') for lemma in hypernym.lemmas())\n",
    "    return list(broader_terms)\n",
    "\n",
    "# Expand keywords using broader terms\n",
    "def expand_keywords(keywords_list):\n",
    "    expanded_keywords = set()\n",
    "    for keyword in keywords_list:\n",
    "        expanded_keywords.add(keyword)\n",
    "        broader_terms = get_broader_terms(keyword)\n",
    "        expanded_keywords.update(broader_terms)\n",
    "    logging.info(f\"Expanded Keywords: {expanded_keywords}\")\n",
    "    return list(expanded_keywords)\n",
    "\n",
    "# Function to generate medical condition keywords from patient data using OpenAI's GPT\n",
    "async def generate_keywords(patient_data):\n",
    "    \"\"\"\n",
    "    Generate medical condition keywords from patient data using OpenAI's API asynchronously.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        f\"Extract specific medical condition keywords from the following patient data. \"\n",
    "        f\"Focus on diseases, disorders, and only medical conditions. \"\n",
    "        f\"Exclude general terms and all non-medical information and terms.\\n\"\n",
    "        f\"Patient Data:\\n{patient_data}\\n\\n\"\n",
    "        f\"Medical Conditions (comma-separated):\"\n",
    "    )\n",
    "    try:\n",
    "        response = await client.chat.completions.create(\n",
    "            model='gpt-3.5-turbo',\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=60,\n",
    "            temperature=0.5\n",
    "        )\n",
    "        # Collect token usage for gpt-3.5-turbo\n",
    "        usage = response.usage\n",
    "        token_usage['gpt-3.5-turbo']['prompt_tokens'] += usage.prompt_tokens\n",
    "        token_usage['gpt-3.5-turbo']['completion_tokens'] += usage.completion_tokens\n",
    "        token_usage['gpt-3.5-turbo']['total_tokens'] += usage.total_tokens\n",
    "        \n",
    "        keywords_text = response.choices[0].message.content.strip()\n",
    "        keywords_list = [kw.strip() for kw in keywords_text.split(',') if kw.strip()]\n",
    "        \n",
    "        # Remove general terms\n",
    "        general_terms = {'consent', 'permission', 'therapy', 'treatment', 'medical procedure', 'medical science',\n",
    "                         'hospital room', 'room', 'palliative care', 'clinical trial', 'surgery', 'radiotherapy',\n",
    "                         'drug treatment', 'ecog performance status'}\n",
    "        keywords_list = [kw for kw in keywords_list if kw.lower() not in general_terms]\n",
    "        \n",
    "        logging.info(f\"Generated Keywords: {keywords_list}\")\n",
    "        \n",
    "        # Expand the keywords using broader terms\n",
    "        expanded_keywords = expand_keywords(keywords_list)\n",
    "        return expanded_keywords\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error generating keywords: {e}\")\n",
    "        print(f\"Error generating keywords: {e}\")\n",
    "        return []\n",
    "\n",
    "# Function to read NCT Numbers from CSV\n",
    "def load_nct_numbers(csv_file_path):\n",
    "    \"\"\"\n",
    "    Load NCT Numbers from a CSV file.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    nct_numbers = df['NCT Number'].dropna().unique().tolist()\n",
    "    logging.info(f\"Loaded {len(nct_numbers)} NCT Numbers from CSV.\")\n",
    "    return nct_numbers\n",
    "\n",
    "def fetch_trials_by_nct_numbers(nct_numbers):\n",
    "    \"\"\"\n",
    "    Fetch trial details for given NCT Numbers.\n",
    "    \"\"\"\n",
    "    base_url = 'https://clinicaltrials.gov/api/v2/studies'\n",
    "    trials_data = []\n",
    "    batch_size = 20  # Reduced batch size\n",
    "\n",
    "    # Load cached trials if available\n",
    "    cache_file = 'trials_cache.json'\n",
    "    if os.path.exists(cache_file):\n",
    "        with open(cache_file, 'r') as f:\n",
    "            try:\n",
    "                cached_trials = json.load(f)\n",
    "            except json.JSONDecodeError:\n",
    "                cached_trials = []\n",
    "        cached_nct_ids = set(trial['protocolSection']['identificationModule']['nctId'] for trial in cached_trials)\n",
    "    else:\n",
    "        cached_trials = []\n",
    "        cached_nct_ids = set()\n",
    "\n",
    "    # NCT IDs that need to be fetched\n",
    "    nct_numbers_to_fetch = [nct for nct in nct_numbers if nct not in cached_nct_ids]\n",
    "\n",
    "    nct_numbers_not_found = []\n",
    "\n",
    "    for i in range(0, len(nct_numbers_to_fetch), batch_size):\n",
    "        batch_nct_ids = nct_numbers_to_fetch[i:i+batch_size]\n",
    "        params = {\n",
    "            'format': 'json',\n",
    "            'filter.ids': ','.join(batch_nct_ids),\n",
    "            'fields': (\n",
    "                'protocolSection.identificationModule.nctId,'\n",
    "                'protocolSection.identificationModule.briefTitle,'\n",
    "                'protocolSection.identificationModule.officialTitle,'\n",
    "                'protocolSection.conditionsModule.conditions,'\n",
    "                'protocolSection.descriptionModule.briefSummary,'\n",
    "                'protocolSection.eligibilityModule.eligibilityCriteria'\n",
    "            )\n",
    "        }\n",
    "        response = requests.get(base_url, params=params)\n",
    "        if response.status_code != 200:\n",
    "            logging.error(f\"Error fetching data: HTTP {response.status_code}\")\n",
    "            print(f\"Error fetching data for NCT IDs: {batch_nct_ids}\")\n",
    "            print(f\"HTTP Status Code: {response.status_code}\")\n",
    "            print(f\"Response content: {response.text}\")\n",
    "            continue\n",
    "        data = response.json()\n",
    "        studies = data.get('studies', [])\n",
    "        if not studies:\n",
    "            logging.warning(f\"No studies found for NCT IDs: {batch_nct_ids}\")\n",
    "            nct_numbers_not_found.extend(batch_nct_ids)\n",
    "        else:\n",
    "            fetched_nct_ids = [study['protocolSection']['identificationModule']['nctId'] for study in studies]\n",
    "            not_found_ids = set(batch_nct_ids) - set(fetched_nct_ids)\n",
    "            nct_numbers_not_found.extend(not_found_ids)\n",
    "            trials_data.extend(studies)\n",
    "        time.sleep(1)  # Be polite and avoid hitting the API too hard\n",
    "\n",
    "    # Combine cached and newly fetched trials\n",
    "    all_trials = cached_trials + trials_data\n",
    "\n",
    "    # Remove duplicates\n",
    "    all_trials_unique = {trial['protocolSection']['identificationModule']['nctId']: trial for trial in all_trials}\n",
    "    all_trials = list(all_trials_unique.values())\n",
    "\n",
    "    # Save the combined trials to cache\n",
    "    with open(cache_file, 'w') as f:\n",
    "        json.dump(all_trials, f)\n",
    "\n",
    "    logging.info(f\"Fetched {len(trials_data)} new trials. Total trials: {len(all_trials)}\")\n",
    "\n",
    "    if nct_numbers_not_found:\n",
    "        logging.warning(f\"Could not find data for {len(nct_numbers_not_found)} NCT IDs: {nct_numbers_not_found}\")\n",
    "        print(f\"Could not find data for the following NCT IDs:\")\n",
    "        print(nct_numbers_not_found)\n",
    "\n",
    "    return all_trials\n",
    "\n",
    "# Function to filter trials based on condition keywords\n",
    "def filter_trials_by_conditions(trials, keywords, include_solid_tumor_markers=False):\n",
    "    \"\"\"\n",
    "    Filter trials based on condition keywords and optionally solid tumor markers.\n",
    "    \"\"\"\n",
    "    filtered_trials = []\n",
    "    keyword_set = set(kw.lower() for kw in keywords)\n",
    "    if include_solid_tumor_markers:\n",
    "        keyword_set.add('solid tumor')\n",
    "    for trial in trials:\n",
    "        conditions = trial.get('protocolSection', {}).get('conditionsModule', {}).get('conditions', [])\n",
    "        # Expand conditions with synonyms and broader terms\n",
    "        condition_terms = set()\n",
    "        for cond in conditions:\n",
    "            cond_lower = cond.lower()\n",
    "            condition_terms.add(cond_lower)\n",
    "            condition_terms.update(get_broader_terms(cond_lower))\n",
    "        if condition_terms & keyword_set:\n",
    "            filtered_trials.append(trial)\n",
    "    logging.info(f\"Filtered {len(filtered_trials)} trials based on condition keywords.\")\n",
    "    return filtered_trials\n",
    "\n",
    "# Function to extract relevant information from each study\n",
    "def extract_trial_info(study):\n",
    "    \"\"\"\n",
    "    Extract relevant information from a study.\n",
    "    \"\"\"\n",
    "    protocol_section = study.get('protocolSection', {})\n",
    "    identification_module = protocol_section.get('identificationModule', {})\n",
    "    description_module = protocol_section.get('descriptionModule', {})\n",
    "    conditions_module = protocol_section.get('conditionsModule', {})\n",
    "    eligibility_module = protocol_section.get('eligibilityModule', {})\n",
    "\n",
    "    nct_id = identification_module.get('nctId', 'N/A')\n",
    "    brief_title = identification_module.get('briefTitle', 'N/A')\n",
    "    official_title = identification_module.get('officialTitle', 'N/A')\n",
    "    conditions = conditions_module.get('conditions', [])\n",
    "    brief_summary = description_module.get('briefSummary', 'N/A')\n",
    "    eligibility_criteria = eligibility_module.get('eligibilityCriteria', 'N/A')\n",
    "\n",
    "    # Clean and format text fields\n",
    "    brief_summary = clean_text(brief_summary)\n",
    "    eligibility_criteria = clean_text(eligibility_criteria)\n",
    "    # Concatenate conditions if multiple\n",
    "    conditions_str = '; '.join(conditions) if isinstance(conditions, list) else conditions\n",
    "\n",
    "    trial_info = {\n",
    "        'nct_id': nct_id,\n",
    "        'title': official_title or brief_title,\n",
    "        'conditions': conditions_str,\n",
    "        'brief_summary': brief_summary,\n",
    "        'eligibility_criteria': eligibility_criteria\n",
    "    }\n",
    "    return trial_info\n",
    "\n",
    "# Asynchronous function to summarize eligibility criteria\n",
    "async def summarize_eligibility_criteria(eligibility_criteria):\n",
    "    prompt = (\n",
    "        f\"Please summarize the following eligibility criteria in 500 characters or less:\\n\\n\"\n",
    "        f\"{eligibility_criteria}\"\n",
    "    )\n",
    "    try:\n",
    "        response = await client.chat.completions.create(\n",
    "            model='gpt-3.5-turbo',\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=150,\n",
    "            temperature=0.5\n",
    "        )\n",
    "        # Collect token usage for gpt-3.5-turbo\n",
    "        usage = response['usage']\n",
    "        token_usage['gpt-3.5-turbo']['prompt_tokens'] += usage.get('prompt_tokens', 0)\n",
    "        token_usage['gpt-3.5-turbo']['completion_tokens'] += usage.get('completion_tokens', 0)\n",
    "        token_usage['gpt-3.5-turbo']['total_tokens'] += usage.get('total_tokens', 0)\n",
    "        \n",
    "        summary = response['choices'][0]['message']['content'].strip()\n",
    "        return summary\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error summarizing eligibility criteria: {e}\")\n",
    "        return eligibility_criteria  # Fallback to original if summarization fails\n",
    "\n",
    "# Asynchronous function to evaluate patient eligibility for a trial using OpenAI's GPT\n",
    "async def match_patient_to_trial_async(patient_data, eligibility_criteria):\n",
    "    prompt = (\n",
    "        f\"As a medical expert, assess the patient's eligibility for the clinical trial based on the eligibility criteria below.\\n\"\n",
    "        f\"Assume that any missing information is favorable to the patient's eligibility unless it is critical for safety or efficacy.\\n\"\n",
    "        f\"Patient Information:\\n{patient_data}\\n\"\n",
    "        f\"Eligibility Criteria:\\n{eligibility_criteria}\\n\"\n",
    "        f\"Provide a match score from 0% to 100% indicating how well the patient matches the eligibility criteria based on the available information. If anything matches exclusion criteria, it's 'Not Eligible'.\\n\"\n",
    "        f\"Conclude 'Eligible', 'Not Eligible', or 'More Info Needed'.\\n\"\n",
    "        f\"Explain briefly why the patient is eligible or not.\\n\"\n",
    "        f\"Please output in the following format:\\n\"\n",
    "        f\"Match Score: [percentage]\\n\"\n",
    "        f\"Eligibility: [Eligible / Not Eligible / More Info Needed]\\n\"\n",
    "        f\"Reason:\\n[Your brief explanation here]\"\n",
    "    )\n",
    "    try:\n",
    "        from openai import AsyncOpenAI\n",
    "        client = AsyncOpenAI()\n",
    "        response = await client.chat.completions.create(\n",
    "            model='gpt-4',\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=300,\n",
    "            temperature=0\n",
    "        )\n",
    "       # Collect token usage for gpt-4\n",
    "        usage = response.usage\n",
    "        token_usage['gpt-4']['prompt_tokens'] += usage.prompt_tokens\n",
    "        token_usage['gpt-4']['completion_tokens'] += usage.completion_tokens\n",
    "        token_usage['gpt-4']['total_tokens'] += usage.total_tokens\n",
    "        result = response.choices[0].message.content.strip()\n",
    "        logging.info(f\"OpenAI API response for trial: {result}\")\n",
    "\n",
    "        # Parse the output\n",
    "        match_score = 0.0\n",
    "        eligibility = 'Unknown'\n",
    "        reason = ''\n",
    "        lines = result.split('\\n')\n",
    "        for line in lines:\n",
    "            if line.startswith('Match Score:'):\n",
    "                match_score_text = line[len('Match Score:'):].replace('%', '').strip()\n",
    "                try:\n",
    "                    match_score = float(match_score_text)\n",
    "                except ValueError:\n",
    "                    match_score = 0.0\n",
    "            elif line.startswith('Eligibility:'):\n",
    "                eligibility = line[len('Eligibility:'):].strip()\n",
    "            elif line.startswith('Reason:'):\n",
    "                reason_index = lines.index(line)\n",
    "                reason = '\\n'.join(lines[reason_index+1:]).strip()\n",
    "                break  # No need to parse further\n",
    "\n",
    "        trial_data = {\n",
    "            'match_result': result,\n",
    "            'score': match_score,\n",
    "            'eligibility': eligibility,\n",
    "            'reason': reason\n",
    "        }\n",
    "        return trial_data\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in match_patient_to_trial_async: {e}\")\n",
    "        return {\n",
    "            'match_result': '',\n",
    "            'score': 0.0,\n",
    "            'eligibility': 'Not Eligible',\n",
    "            'reason': ''\n",
    "        }\n",
    "\n",
    "# Initialize tokenizer\n",
    "encoding = tiktoken.encoding_for_model('text-embedding-ada-002')\n",
    "\n",
    "def count_tokens(text, model='text-embedding-ada-002'):\n",
    "    tokens = encoding.encode(text)\n",
    "    return len(tokens)\n",
    "import numpy as np\n",
    "\n",
    "async def get_embedding(text):\n",
    "    token_count = count_tokens(text)\n",
    "    token_usage['text-embedding-ada-002']['total_tokens'] += token_count\n",
    "    from openai import AsyncOpenAI\n",
    "    client= AsyncOpenAI()\n",
    "    response = await client.embeddings.create(\n",
    "        input=[text],  # Input as list\n",
    "        model='text-embedding-ada-002'\n",
    "    )\n",
    "    # Access embedding using dot notation\n",
    "    return response.data[0].embedding\n",
    "\n",
    "async def rank_trials_with_embeddings(patient_data, trials):\n",
    "    patient_embedding = await get_embedding(patient_data)\n",
    "    trial_infos = []\n",
    "    for trial in trials:\n",
    "        trial_info = extract_trial_info(trial)\n",
    "        eligibility_criteria = trial_info['eligibility_criteria']\n",
    "        if not eligibility_criteria or eligibility_criteria == 'N/A':\n",
    "            continue\n",
    "        trial_info['eligibility_criteria_summary'] = eligibility_criteria  # Use full criteria\n",
    "        trial_infos.append(trial_info)\n",
    "    \n",
    "    logging.info(f\"Matching patient to {len(trial_infos)} trials using embeddings.\")\n",
    "    print(\"Matching trials using embeddings...\")\n",
    "    trial_results = []\n",
    "    \n",
    "    for trial_info in tqdm(trial_infos):\n",
    "        trial_embedding = await get_embedding(trial_info['eligibility_criteria_summary'])\n",
    "        similarity = np.dot(patient_embedding, trial_embedding) / (np.linalg.norm(patient_embedding) * np.linalg.norm(trial_embedding))\n",
    "        trial_data = {\n",
    "            'nct_id': trial_info['nct_id'],\n",
    "            'title': trial_info['title'],\n",
    "            'similarity_score': similarity,\n",
    "            'eligibility_criteria': trial_info['eligibility_criteria_summary']\n",
    "        }\n",
    "        trial_results.append(trial_data)\n",
    "    return trial_results\n",
    "\n",
    "\n",
    "# Function to generate patient profile paragraph\n",
    "def generate_patient_paragraph(patient_info):\n",
    "    # Convert the patient_info dictionary to a formatted string\n",
    "    formatted_info = '\\n'.join([f\"{key}: {value}\" for key, value in patient_info.items()])\n",
    "    \n",
    "    prompt = (\n",
    "        \"Create a detailed patient profile paragraph based solely on the following information from the CSV. \"\n",
    "        \"Do not add any additional information or make any assumptions beyond what is provided. \"\n",
    "        \"Ensure that the paragraph includes a statement where the patient gives consent for their data to be used.\\n\\n\"\n",
    "        f\"{formatted_info}\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that creates patient profiles based only on provided data.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=300,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "        # Access the response correctly using attribute notation\n",
    "        paragraph = response.choices[0].message.content.strip()\n",
    "        return paragraph\n",
    "    except Exception as e:\n",
    "        return f\"Error generating paragraph: {str(e)}\"\n",
    "\n",
    "# Widgets for the Voila app\n",
    "# Create a file upload widget\n",
    "upload_widget = widgets.FileUpload(\n",
    "    accept='.csv',  # Accept only CSV files\n",
    "    multiple=False,  # Do not allow multiple uploads\n",
    "    description='Upload CSV'\n",
    ")\n",
    "\n",
    "# Create patient data textarea (from code 1)\n",
    "patient_data_textarea = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Patient data will appear here...',\n",
    "    description='Patient Data:',\n",
    "    layout=widgets.Layout(width='800px', height='200px')\n",
    ")\n",
    "\n",
    "include_solid_tumor_checkbox = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Include Solid Tumor Markers',\n",
    "    disabled=False,\n",
    "    indent=False\n",
    ")\n",
    "\n",
    "submit_button = widgets.Button(\n",
    "    description='Submit',\n",
    "    button_style='success',\n",
    "    tooltip='Click to submit',\n",
    "    icon='check'\n",
    ")\n",
    "\n",
    "# Create an output area to display results\n",
    "output_area = widgets.Output()\n",
    "\n",
    "# Attach the callback to the submit button\n",
    "def on_submit_button_clicked(b):\n",
    "    with output_area:\n",
    "        clear_output()\n",
    "        patient_data = patient_data_textarea.value.strip()\n",
    "        include_solid_tumor_markers = include_solid_tumor_checkbox.value\n",
    "        if not patient_data:\n",
    "            print(\"Please enter patient data.\")\n",
    "            return\n",
    "\n",
    "        print(\"Generating keywords from patient data...\")\n",
    "        loop = asyncio.get_event_loop()\n",
    "        keywords = loop.run_until_complete(generate_keywords(patient_data))\n",
    "        if not keywords:\n",
    "            print(\"Failed to generate keywords.\")\n",
    "            return\n",
    "        print(\"Keywords generated:\", ', '.join(keywords))\n",
    "\n",
    "        # Filter trials based on condition keywords\n",
    "        print(\"\\nFiltering trials based on condition keywords...\")\n",
    "        filtered_trials = filter_trials_by_conditions(trials_data, keywords, include_solid_tumor_markers)\n",
    "        print(f\"Total trials fetched: {total_trials_fetched}\")\n",
    "        print(f\"Filtered {len(filtered_trials)} trials based on condition keywords.\")\n",
    "\n",
    "        if not filtered_trials:\n",
    "            print(\"No trials found matching the condition keywords.\")\n",
    "            return\n",
    "\n",
    "        # Limit the number of trials to process to manage API usage\n",
    "        max_trials_to_process = 20  # Adjust as needed\n",
    "        trials_to_process = filtered_trials[:max_trials_to_process]\n",
    "\n",
    "        print(f\"\\nMatching patient to {len(trials_to_process)} trials (this may take some time)...\")\n",
    "\n",
    "        # Run the ranking and matching asynchronously\n",
    "        try:\n",
    "            trial_results = loop.run_until_complete(process_trials_concurrently(patient_data, trials_to_process))\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during trial matching: {e}\")\n",
    "            return\n",
    "\n",
    "        if not trial_results:\n",
    "            print(\"No trials matched the patient data.\")\n",
    "            return\n",
    "\n",
    "        # Sort trials by score (higher score first)\n",
    "        trial_results.sort(key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "        # Display all matching trials\n",
    "        print(\"\\nMatching Clinical Trials:\")\n",
    "        for trial in trial_results:\n",
    "            print(f\"Title: {trial['title']}\")\n",
    "            print(f\"NCT ID: {trial['nct_id']}\")\n",
    "            print(f\"Eligibility: {trial['eligibility']}\")\n",
    "            print(f\"Score: {trial['score']}%\")\n",
    "            print(f\"Reason:\\n{trial['reason']}\\n\")\n",
    "            print(\"-\" * 80)\n",
    "\n",
    "        # Calculate costs\n",
    "        costs = {}\n",
    "\n",
    "        # Prices per 1,000 tokens\n",
    "        costs['gpt-4'] = (\n",
    "            (token_usage['gpt-4']['prompt_tokens'] / 1000) * 0.03 +  # Prompt tokens\n",
    "            (token_usage['gpt-4']['completion_tokens'] / 1000) * 0.06  # Completion tokens\n",
    "        )\n",
    "\n",
    "        costs['gpt-3.5-turbo'] = (\n",
    "            (token_usage['gpt-3.5-turbo']['total_tokens'] / 1000) * 0.002\n",
    "        )\n",
    "\n",
    "        costs['text-embedding-ada-002'] = (\n",
    "            (token_usage['text-embedding-ada-002']['total_tokens'] / 1000) * 0.0004\n",
    "        )\n",
    "\n",
    "        total_cost = costs.get('gpt-4', 0) + costs.get('gpt-3.5-turbo', 0) + costs['text-embedding-ada-002']\n",
    "\n",
    "        # Print token usage and costs\n",
    "        print(\"\\nToken Usage:\")\n",
    "        if token_usage['gpt-4']['total_tokens'] > 0:\n",
    "            print(f\"gpt-4:\")\n",
    "            print(f\"  Prompt Tokens: {token_usage['gpt-4']['prompt_tokens']}\")\n",
    "            print(f\"  Completion Tokens: {token_usage['gpt-4']['completion_tokens']}\")\n",
    "            print(f\"  Total Tokens: {token_usage['gpt-4']['total_tokens']}\")\n",
    "            print(f\"  Cost: ${costs['gpt-4']:.6f}\")\n",
    "\n",
    "        if token_usage['gpt-3.5-turbo']['total_tokens'] > 0:\n",
    "            print(f\"gpt-3.5-turbo:\")\n",
    "            print(f\"  Total Tokens: {token_usage['gpt-3.5-turbo']['total_tokens']}\")\n",
    "            print(f\"  Cost: ${costs['gpt-3.5-turbo']:.6f}\")\n",
    "\n",
    "        print(f\"text-embedding-ada-002:\")\n",
    "        print(f\"  Total Tokens: {token_usage['text-embedding-ada-002']['total_tokens']}\")\n",
    "        print(f\"  Cost: ${costs['text-embedding-ada-002']:.6f}\")\n",
    "\n",
    "        print(f\"\\nEstimated Total Cost: ${total_cost:.6f}\")\n",
    "\n",
    "submit_button.on_click(on_submit_button_clicked)\n",
    "\n",
    "# Function to handle file upload and processing\n",
    "def handle_file_upload(change):\n",
    "    with output_area:\n",
    "        clear_output()\n",
    "        if upload_widget.value:\n",
    "            # Get the uploaded file\n",
    "            uploaded_file = next(iter(upload_widget.value.values()))\n",
    "            content = uploaded_file['content']\n",
    "            try:\n",
    "                # Read CSV content into DataFrame using io.BytesIO\n",
    "                df = pd.read_csv(io.BytesIO(content))\n",
    "                if df.empty:\n",
    "                    print(\"The uploaded CSV file is empty.\")\n",
    "                    return\n",
    "                if len(df) != 1:\n",
    "                    print(\"Please upload a CSV file with exactly one patient record.\")\n",
    "                    return\n",
    "                \n",
    "                # Convert the single row to a dictionary\n",
    "                patient_info = df.iloc[0].to_dict()\n",
    "                print(\"**Patient Information:**\")\n",
    "                display(df)\n",
    "                \n",
    "                # Generate paragraph using OpenAI\n",
    "                print(\"\\n**Generating patient profile paragraph...**\\n\")\n",
    "                paragraph = generate_patient_paragraph(patient_info)\n",
    "                print(\"**Generated Patient Profile:**\\n\")\n",
    "                print(paragraph)\n",
    "                \n",
    "                # Insert the paragraph into the patient data textarea\n",
    "                patient_data_textarea.value = paragraph\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing CSV file: {e}\")\n",
    "\n",
    "# Link the upload event to the handler function\n",
    "upload_widget.observe(handle_file_upload, names='value')\n",
    "\n",
    "# Load NCT Numbers and trial data at the beginning\n",
    "csv_file_path = 'table_trials_andy.csv'  # Replace with your CSV file path\n",
    "nct_numbers = load_nct_numbers(csv_file_path)\n",
    "trials_data = fetch_trials_by_nct_numbers(nct_numbers)\n",
    "total_trials_fetched = len(trials_data)\n",
    "\n",
    "# Display the widgets\n",
    "display(\n",
    "    widgets.VBox([\n",
    "        upload_widget,\n",
    "        patient_data_textarea,\n",
    "        include_solid_tumor_checkbox,\n",
    "        submit_button,\n",
    "        output_area\n",
    "    ])\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
